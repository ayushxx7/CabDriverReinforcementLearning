{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Cab-Driver Agent","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nimport numpy as np\nimport random\nimport math\nfrom collections import deque\nimport collections\nimport pickle\nimport time\n\n# for building DQN model\nfrom keras import layers\nfrom keras import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam\n\n# for plotting graphs\nimport matplotlib.pyplot as plt\n\n# Import the environment\nfrom env_py import CabDriver","metadata":{"execution":{"iopub.status.busy":"2022-04-19T07:28:56.969463Z","iopub.execute_input":"2022-04-19T07:28:56.969826Z","iopub.status.idle":"2022-04-19T07:29:03.848517Z","shell.execute_reply.started":"2022-04-19T07:28:56.969732Z","shell.execute_reply":"2022-04-19T07:29:03.847613Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 1. Load the Time Matrix","metadata":{}},{"cell_type":"code","source":"# Loading the time matrix provided\nTime_matrix = np.load(\"../input/tm-npy-kaggle/TM.npy\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T07:29:03.850104Z","iopub.execute_input":"2022-04-19T07:29:03.850385Z","iopub.status.idle":"2022-04-19T07:29:03.866058Z","shell.execute_reply.started":"2022-04-19T07:29:03.850346Z","shell.execute_reply":"2022-04-19T07:29:03.865335Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### Check what the max, min and mean time values are. This will help us in defining the 'next_step' function in the Environment.","metadata":{}},{"cell_type":"code","source":"print(type(Time_matrix))\nprint(Time_matrix.max())\nprint(Time_matrix.min())\nprint(Time_matrix.mean())\nprint(Time_matrix.var())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-19T07:29:09.291787Z","iopub.execute_input":"2022-04-19T07:29:09.292342Z","iopub.status.idle":"2022-04-19T07:29:09.298607Z","shell.execute_reply.started":"2022-04-19T07:29:09.292300Z","shell.execute_reply":"2022-04-19T07:29:09.297818Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### Since the max time is 11 hours between any 2 points, the next state of the cab driver may increase at most by  1 day.","metadata":{}},{"cell_type":"markdown","source":"### 2. Agent Class\n\nIf you are using this framework, you need to fill the following to complete the following code block:\n1. State and Action Size\n2. Hyperparameters\n3. Create a neural-network model in function 'build_model()'\n4. Define epsilon-greedy strategy in function 'get_action()'\n5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n6. Complete the 'train_model()' function with following logic:\n   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n      - Initialise your input and output batch for training the model\n      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n      - Get Q(s', a) values from the last trained model\n      - Update the input batch as your encoded state and output batch as your Q-values\n      - Then fit your DQN model using the updated input and output batch.","metadata":{}},{"cell_type":"code","source":"class DQNAgent:\n    def __init__(self, state_size, action_size):\n        # Define size of state and action\n        self.state_size = state_size\n        self.action_size = action_size\n\n        # Write here: Specify you hyper parameters for the DQN\n        self.discount_factor = 0.95\n        self.learning_rate = 0.01\n        self.epsilon = 1\n        self.epsilon_max = 1\n        self.epsilon_decay = -0.0005 #for 15k\n        #self.epsilon_decay = -0.00015 #for 20k\n        self.epsilon_min = 0.00001\n        \n        self.batch_size = 32\n\n        # create replay memory using deque\n        self.memory = deque(maxlen=2000)\n\n        # Initialize the value of the states tracked\n        self.states_tracked = []\n        \n        # We are going to track state [0,0,0] and action (0,2) at index 2 in the action space.\n        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n\n        # create main model and target model\n        self.model = self.build_model()\n\n    # approximate Q function using Neural Network\n    def build_model(self):\n        \"\"\"\n        Function that takes in the agent and constructs the network\n        to train it\n        @return model\n        @params agent\n        \"\"\"\n#         # detect and init the TPU\n#         tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n#         # instantiate a distribution strategy\n#         tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n#         input_shape = self.state_size\n        \n#         with tpu_strategy.scope():\n        model = Sequential()\n        # Write your code here: Add layers to your neural nets       \n        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n        # the output layer: output is of size num_actions\n        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n        model.summary\n        return model\n\n    def get_action(self, state, possible_actions_index, actions):\n        \"\"\"\n        get action in a state according to an epsilon-greedy approach\n        possible_actions_index, actions are the 'ride requests' that teh driver got.\n        \"\"\"        \n        # get action from model using epsilon-greedy policy\n        # Decay in Îµ after each episode       \n        if np.random.rand() <= self.epsilon:\n            # explore: choose a random action from the ride requests\n            return random.choice(possible_actions_index)\n        else:\n            # choose the action with the highest q(s, a)\n            # the first index corresponds to the batch size, so\n            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n\n            # Use the model to predict the Q_values.\n            q_value = self.model.predict(state)\n\n            # truncate the array to only those actions that are part of the ride  requests.\n            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n\n            return possible_actions_index[np.argmax(q_vals_possible)]\n\n    def append_sample(self, state, action_index, reward, next_state, done):\n        \"\"\"appends the new agent run output to replay buffer\"\"\"\n        self.memory.append((state, action_index, reward, next_state, done))\n        \n    # pick samples randomly from replay memory (with batch_size) and train the network\n    def train_model(self):\n        \"\"\" \n        Function to train the model on eacg step run.\n        Picks the random memory events according to batch size and \n        runs it through the network to train it.\n        \"\"\"\n        if len(self.memory) > self.batch_size:\n            # Sample batch from the memory\n            mini_batch = random.sample(self.memory, self.batch_size)\n            # initialise two matrices - update_input and update_output\n            update_input = np.zeros((self.batch_size, self.state_size))\n            update_output = np.zeros((self.batch_size, self.state_size))\n            actions, rewards, done = [], [], []\n\n            # populate update_input and update_output and the lists rewards, actions, done\n            for i in range(self.batch_size):\n                state, action, reward, next_state, done_boolean = mini_batch[i]\n                update_input[i] = env.state_encod_arch1(state)     \n                actions.append(action)\n                rewards.append(reward)\n                update_output[i] = env.state_encod_arch1(next_state)\n                done.append(done_boolean)\n\n            # predict the target q-values from states s\n            target = self.model.predict(update_input)\n            # target for q-network\n            target_qval = self.model.predict(update_output)\n\n\n            # update the target values\n            for i in range(self.batch_size):\n                if done[i]:\n                    target[i][actions[i]] = rewards[i]\n                else: # non-terminal state\n                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n            # model fit\n            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n            \n    def save_tracking_states(self):\n        # Use the model to predict the q_value of the state we are tacking.\n        q_value = self.model.predict(self.track_state)\n        \n        # Grab the q_value of the action index that we are tracking.\n        self.states_tracked.append(q_value[0][2])\n        \n    def dump_obj(self, obj, name):\n        import joblib\n        \n        joblib.dump(obj, name)\n        \n    def save_test_states(self):\n        # Use the model to predict the q_value of the state we are tacking.\n        q_value = self.model.predict(self.track_state)\n        \n        # Grab the q_value of the action index that we are tracking.\n        self.states_test.append(q_value[0][2])\n\n    def save(self, name):\n        with open(name, 'wb') as file:  \n            pickle.dump(self.model, file,pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T07:29:13.936285Z","iopub.execute_input":"2022-04-19T07:29:13.936571Z","iopub.status.idle":"2022-04-19T07:29:13.958259Z","shell.execute_reply.started":"2022-04-19T07:29:13.936539Z","shell.execute_reply":"2022-04-19T07:29:13.957045Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 3. DQN block","metadata":{}},{"cell_type":"markdown","source":"for episode in range(Episodes):\n\n    # Write code here\n    # Call the environment\n    # Call all the initialised variables of the environment\n    \n\n    #Call the DQN agent\n    \n    \n    while !terminal_state:\n        \n        # Write your code here\n        # 1. Pick epsilon-greedy action from possible actions for the current state\n        # 2. Evaluate your reward and next state\n        # 3. Append the experience to the memory\n        # 4. Train the model by calling function agent.train_model\n        # 5. Keep a track of rewards, Q-values, loss\n        ","metadata":{}},{"cell_type":"code","source":"episode_time = 24*30 # 30 days before which car has to be recharged\nn_episodes = 5000\nm = 5\nt = 24\nd = 7\n\n# Invoke Env class\nenv = CabDriver()\naction_space, state_space, state = env.reset()\n\n# Set up state and action sizes.\nstate_size = m+t+d\naction_size = len(action_space)\n\n# Invoke agent class\nagent = DQNAgent(action_size=action_size, state_size=state_size)\n\n# to store rewards in each episode\nrewards_per_episode, episodes = [], []\n# Rewards for state [0,0,0] being tracked.\nrewards_init_state = []","metadata":{"execution":{"iopub.status.busy":"2022-04-19T07:29:15.461712Z","iopub.execute_input":"2022-04-19T07:29:15.461997Z","iopub.status.idle":"2022-04-19T07:29:15.592003Z","shell.execute_reply.started":"2022-04-19T07:29:15.461968Z","shell.execute_reply":"2022-04-19T07:29:15.591011Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### 4. Run the episodes, build up replay buffer and train the model.\n### Note:\n#### The moment total episode time exceeds 720 (30 days), we ignore the most recent ride and do NOT save that experience in the replay memory\n#### The init state is randomly picked from the state space for each episode","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nstart_time = time.time()\nscore_tracked = []\n\nfor episode in tqdm(range(n_episodes)):\n\n    done = False\n    score = 0\n    track_reward = False\n\n    # reset at the start of each episode\n    env = CabDriver()\n    action_space, state_space, state = env.reset()\n    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n    initial_state = env.state_init\n\n\n    total_time = 0  # Total time driver rode in this episode\n    while not done:\n        # 1. Get a list of the ride requests driver got.\n        possible_actions_indices, actions = env.requests(state)\n        # 2. Pick epsilon-greedy action from possible actions for the current state.\n        action = agent.get_action(state, possible_actions_indices, actions)\n\n        # 3. Evaluate your reward and next state\n        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n        # 4. Total time driver rode in this episode\n        total_time += step_time\n        if (total_time > episode_time):\n            # if ride does not complete in stipu;ated time skip\n            # it and move to next episode.\n            done = True\n        else:\n            # 5. Append the experience to the memory\n            agent.append_sample(state, action, reward, next_state, done)\n            # 6. Train the model by calling function agent.train_model\n            agent.train_model()\n            # 7. Keep a track of rewards, Q-values, loss\n            score += reward\n            state = next_state\n\n    # store total reward obtained in this episode\n    rewards_per_episode.append(score)\n    episodes.append(episode)\n    \n\n    # epsilon decay\n    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n\n    # every 10 episodes:\n    if ((episode + 1) % 10 == 0):\n        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n                                                                         score,\n                                                                         len(agent.memory),\n                                                                         agent.epsilon, total_time))\n    # Save the Q_value of the state, action pair we are tracking\n    if ((episode + 1) % 5 == 0):\n        agent.save_tracking_states()\n\n    # Total rewards per episode\n    score_tracked.append(score)\n\n    if(episode % 100 == 0):\n        print(\"Saving Model {}\".format(episode))\n        agent.model.save(f\"model_{episode}.pkl\")\n#         agent.dump_obj(agent.model, f\"model_{episode}.pkl\")\n        agent.dump_obj(agent.states_tracked, f\"states_tracked_{episode}.pkl\")\n    \nelapsed_time = time.time() - start_time\nprint(elapsed_time)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-19T07:43:58.807225Z","iopub.execute_input":"2022-04-19T07:43:58.807493Z","iopub.status.idle":"2022-04-19T07:46:02.012196Z","shell.execute_reply.started":"2022-04-19T07:43:58.807465Z","shell.execute_reply":"2022-04-19T07:46:02.010816Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### The 'total_time' above includes the 'last ride' time also in each episode. Although it exceeds 24*30 = 720, our code drops the last ride from the replay buffer. So the total ride time per episode is limited to < 720","metadata":{}},{"cell_type":"code","source":"# agent.save(name=\"model_weights.pkl\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tracking Convergence","metadata":{}},{"cell_type":"code","source":"agent.states_tracked","metadata":{"execution":{"iopub.status.busy":"2022-04-19T07:09:04.109584Z","iopub.execute_input":"2022-04-19T07:09:04.110235Z","iopub.status.idle":"2022-04-19T07:09:04.190508Z","shell.execute_reply.started":"2022-04-19T07:09:04.110145Z","shell.execute_reply":"2022-04-19T07:09:04.189544Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Plot the Q-Value convergence for state action pairs","metadata":{}},{"cell_type":"code","source":"plt.figure(0, figsize=(16,7))\nplt.title('Q_value for state [0,0,0]  action (0,2)')\nxaxis = np.asarray(range(0, len(agent.states_tracked)))\nplt.semilogy(xaxis,np.asarray(agent.states_tracked))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We are using log scale because the initial q_values are way to high compared to the steady state value (around 600)","metadata":{}},{"cell_type":"markdown","source":"### 6. Track rewards per episode.","metadata":{}},{"cell_type":"code","source":"score_tracked_sample = [score_tracked[i] for i in range(len(score_tracked)) if (i % 4 == 0)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(0, figsize=(16,7))\nplt.title('Rewards per episode')\nxaxis = np.asarray(range(0, len(score_tracked_sample)))\nplt.plot(xaxis,np.asarray(score_tracked_sample))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see from the above plot that the rewards converge at around 1500. Since the initial state is picked to be random for each episode, some initial states may be less rewarding than others inherently regardless of the model quality.","metadata":{}},{"cell_type":"markdown","source":"#### Epsilon-decay sample function","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\nTry building a similar epsilon-decay function for your model.\n</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\ntime = np.arange(0,15000)\nepsilon = []\nfor i in range(0,15000):\n    epsilon.append((1 - 0.00001) * np.exp(-0.0005 * i))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(time, epsilon)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}